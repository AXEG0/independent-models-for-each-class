{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"First, let's create a synthetic dataset:","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"from collections import Counter\nfrom sklearn.datasets import make_classification\n# create dataframe\nX, y = make_classification(n_samples=1000,\n                           n_features=20,\n                           n_informative=15,\n                           n_redundant=5,\n                           random_state=1)\n# print the data classes info\nprint(f'''Main dataframe:\nNumber of samples: {X.shape[0]}\nNumber of features: {X.shape[1]}\nSamples by class:''')\ncounter = Counter(y)\nfor k, v in counter.items():\n    per = v / len(y) * 100\n    print('Class=%d, Count=%d, Percentage=%.1f%%' % (k, v, per))\n    \n# Main dataframe:\n# Number of samples: 1000\n# Number of features: 20\n# Samples by class:\n# Class=0, Count=501, Percentage=50.1%\n# Class=1, Count=499, Percentage=49.9%","metadata":{"execution":{"iopub.status.busy":"2022-06-01T12:22:22.274107Z","iopub.execute_input":"2022-06-01T12:22:22.275217Z","iopub.status.idle":"2022-06-01T12:22:23.540908Z","shell.execute_reply.started":"2022-06-01T12:22:22.275109Z","shell.execute_reply":"2022-06-01T12:22:23.539689Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"Then, manage with a simple train / test split:","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n# split data to train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.5, stratify=y, random_state=1)\n# show the data classes info\nprint(f'''\\nTrain dataframe:\nNumber of samples: {X_train.shape[0]}\nNumber of features: {X_train.shape[1]}\nSamples by class:''')\ncounter = Counter(y_train)\nfor k, v in counter.items():\n    per = v / len(y_train) * 100\n    print('Class=%d, Count=%d, Percentage=%.1f%%' % (k, v, per))\n\n# Train dataframe:\n# Number of samples: 500\n# Number of features: 20\n# Samples by class:\n# Class=0, Count=251, Percentage=50.2%\n# Class=1, Count=249, Percentage=49.8%","metadata":{"execution":{"iopub.status.busy":"2022-06-01T12:22:23.543531Z","iopub.execute_input":"2022-06-01T12:22:23.544393Z","iopub.status.idle":"2022-06-01T12:22:23.636655Z","shell.execute_reply.started":"2022-06-01T12:22:23.544338Z","shell.execute_reply":"2022-06-01T12:22:23.635050Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"Write a function to fit and evaluate the models:","metadata":{}},{"cell_type":"code","source":"import time\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n# define model evaluator\ndef predictor(model):\n    tic = time.perf_counter()\n    # fit classifier\n    model.fit(X_train, y_train)\n    # get prediction\n    y_pred = model.predict(X_test)\n    # check results\n    accuracy = accuracy_score(y_test, y_pred)\n    matrix = confusion_matrix(y_test, y_pred)\n    # show results\n    print(f'''\\n{type(model).__name__} results:\nAccuracy:{accuracy * 100: 0.1f}%\n{matrix}''')\n    toc = time.perf_counter()\n    print(f\"Processed in {toc - tic: 0.4f} seconds\")","metadata":{"execution":{"iopub.status.busy":"2022-06-01T12:22:23.638501Z","iopub.execute_input":"2022-06-01T12:22:23.639273Z","iopub.status.idle":"2022-06-01T12:22:23.647714Z","shell.execute_reply.started":"2022-06-01T12:22:23.639207Z","shell.execute_reply":"2022-06-01T12:22:23.646443Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"Let's look at the models:","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n# define model a and model b\nmodel_a = RandomForestClassifier(random_state=1)\nmodel_b = KNeighborsClassifier()\n# fit and evaluate models\npredictor(model_a)\npredictor(model_b)\n\n# RandomForestClassifier results:\n# Accuracy: 91.2%\n# [[234  16]\n#  [ 28 222]]\n# Processed in  0.1965 seconds\n#\n# KNeighborsClassifier results:\n# Accuracy: 90.4%\n# [[223  27]\n#  [ 21 229]]\n# Processed in  0.0781 seconds","metadata":{"execution":{"iopub.status.busy":"2022-06-01T12:22:23.652511Z","iopub.execute_input":"2022-06-01T12:22:23.653750Z","iopub.status.idle":"2022-06-01T12:22:24.206797Z","shell.execute_reply.started":"2022-06-01T12:22:23.653668Z","shell.execute_reply":"2022-06-01T12:22:24.204863Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"Let's predict both classes using the \"a\" model:","metadata":{}},{"cell_type":"code","source":"import copy\nimport pandas as pd\n# get prediction of model a\ny_pred_a_01 = model_a.predict(X_test)\n# define dataframe with prediction of model a\ndf_pred_a_01 = copy.copy(X_test)\ndf_pred_a_01 = pd.DataFrame(df_pred_a_01)\ndf_pred_a_01['y'] = y_pred_a_01.tolist()\n# show the data classes info\nprint(f'''\\nDataframe with prediction of model a:\nNumber of samples: {df_pred_a_01.shape[0]}\nNumber of features: {df_pred_a_01.shape[1] - 1}\nSamples by class:''')\ncounter = Counter(y_pred_a_01)\nfor k, v in counter.items():\n    per = v / len(y_pred_a_01) * 100\n    print('Class=%d, Count=%d, Percentage=%.1f%%' % (k, v, per))\n\n# Dataframe with prediction of model a:\n# Number of samples: 500\n# Number of features: 20\n# Samples by class:\n# Class=1, Count=238, Percentage=47.6%\n# Class=0, Count=262, Percentage=52.4%","metadata":{"execution":{"iopub.status.busy":"2022-06-01T12:22:24.211590Z","iopub.execute_input":"2022-06-01T12:22:24.212973Z","iopub.status.idle":"2022-06-01T12:22:24.283465Z","shell.execute_reply.started":"2022-06-01T12:22:24.212911Z","shell.execute_reply":"2022-06-01T12:22:24.282413Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"Split dataframe with prediction of model a to predictions of first and second classes","metadata":{}},{"cell_type":"code","source":"# define with what class model a will operate\nclass_order = 'first'\n# split dataframe with prediction of model a to predictions of first and second classes\nif class_order == 'first':\n    df_pred_a_f = df_pred_a_01[df_pred_a_01['y'] == 0]\n    df_test_b_s = df_pred_a_01[df_pred_a_01['y'] == 1]\nif class_order == 'second':\n    df_pred_a_f = df_pred_a_01[df_pred_a_01['y'] == 1]\n    df_test_b_s = df_pred_a_01[df_pred_a_01['y'] == 0]\n# define test dataframe of model b with predicted samples of chosen class\nX_test_b_s = df_test_b_s.drop(['y'], axis=1)\n# get prediction of model b\ny_pred_b_s = model_b.predict(X_test_b_s)\n# define dataset of prediction of model b\ndf_pred_b_s = copy.copy(X_test_b_s)\ndf_pred_b_s['y'] = y_pred_b_s.tolist()\nprint(f'''\\nDataframe with prediction of model b:\nNumber of samples: {df_pred_b_s.shape[0]}\nNumber of features: {df_pred_b_s.shape[1] - 1}\nSamples by class:''')\ncounter = Counter(y_pred_b_s)\nfor k, v in counter.items():\n    per = v / len(y_pred_b_s) * 100\n    print('Class=%d, Count=%d, Percentage=%.1f%%' % (k, v, per))\n\n# Dataframe with prediction of model b:\n# Number of samples: 238\n# Number of features: 20\n# Samples by class:\n# Class=0, Count=19, Percentage=8.0%\n# Class=1, Count=219, Percentage=92.0%","metadata":{"execution":{"iopub.status.busy":"2022-06-01T12:22:24.287012Z","iopub.execute_input":"2022-06-01T12:22:24.287586Z","iopub.status.idle":"2022-06-01T12:22:24.334854Z","shell.execute_reply.started":"2022-06-01T12:22:24.287549Z","shell.execute_reply":"2022-06-01T12:22:24.333786Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"To calculate the overall accuracy of the two models and compare with how they coped alone combine the results:","metadata":{}},{"cell_type":"code","source":"# add dataframes with predictions of model a and b together\ndf_pred_ab_01 = pd.concat([df_pred_a_f, df_pred_b_s]).sort_index()\ny_pred_ab_01 = df_pred_ab_01['y']\n# get accuracy and confusion matrix of predictions of model a and b\naccuracy_ab_01 = accuracy_score(y_test, y_pred_ab_01)\nmatrix_ab_01 = confusion_matrix(y_test, y_pred_ab_01)\n# show results\nprint(f'''\\nModel a and b results {class_order} class first:\nAccuracy:{accuracy_ab_01 * 100: 0.1f}%\n{matrix_ab_01}''')\n\n# Model a and b results first class first:\n# Accuracy: 90.2%\n# [[241   9]\n#  [ 40 210]]","metadata":{"execution":{"iopub.status.busy":"2022-06-01T12:22:24.336571Z","iopub.execute_input":"2022-06-01T12:22:24.337291Z","iopub.status.idle":"2022-06-01T12:22:24.354829Z","shell.execute_reply.started":"2022-06-01T12:22:24.337221Z","shell.execute_reply":"2022-06-01T12:22:24.352523Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"Let's add the method above into one function and let the model \"b\" now work not with class 1, but with class 0:","metadata":{}},{"cell_type":"code","source":"# Evaluate models on different classes\ndef class_in_order_predictor(class_order='first'):\n    print('\\nCurrent chosen class for model b:', class_order)\n    # get prediction of model a\n    y_pred_a_01 = model_a.predict(X_test)\n    # define dataframe with prediction of model a\n    df_pred_a_01 = copy.copy(X_test)\n    df_pred_a_01 = pd.DataFrame(df_pred_a_01)\n    df_pred_a_01['y'] = y_pred_a_01.tolist()\n    # show the data classes info\n    print(f'''\\nDataframe with prediction of model a:\nNumber of samples: {df_pred_a_01.shape[0]}\nNumber of features: {df_pred_a_01.shape[1] - 1}\nSamples by class:''')\n    counter = Counter(y_pred_a_01)\n    for k, v in counter.items():\n        per = v / len(y_pred_a_01) * 100\n        print('Class=%d, Count=%d, Percentage=%.1f%%' % (k, v, per))\n    # split dataframe with prediction of model a to predictions of first and second classes\n    if class_order == 'first':\n        df_pred_a_f = df_pred_a_01[df_pred_a_01['y'] == 0]\n        df_test_b_s = df_pred_a_01[df_pred_a_01['y'] == 1]\n    if class_order == 'second':\n        df_pred_a_f = df_pred_a_01[df_pred_a_01['y'] == 1]\n        df_test_b_s = df_pred_a_01[df_pred_a_01['y'] == 0]\n    # define test dataframe of model b with predicted samples of second class\n    X_test_b_s = df_test_b_s.drop(['y'], axis=1)\n    # get prediction of model b\n    y_pred_b_s = model_b.predict(X_test_b_s)\n    # define dataset of prediction of model b\n    df_pred_b_s = copy.copy(X_test_b_s)\n    df_pred_b_s['y'] = y_pred_b_s.tolist()\n    print(f'''\\nDataframe with prediction of model b:\nNumber of samples: {df_pred_b_s.shape[0]}\nNumber of features: {df_pred_b_s.shape[1] - 1}\nSamples by class:''')\n    counter = Counter(y_pred_b_s)\n    for k, v in counter.items():\n        per = v / len(y_pred_b_s) * 100\n        print('Class=%d, Count=%d, Percentage=%.1f%%' % (k, v, per))\n    # add dataframes with predictions of model a and b together\n    df_pred_ab_01 = pd.concat([df_pred_a_f, df_pred_b_s]).sort_index()\n    y_pred_ab_01 = df_pred_ab_01['y']\n    # get accuracy and confusion matrix of predictions of model a and b\n    accuracy_ab_01 = accuracy_score(y_test, y_pred_ab_01)\n    matrix_ab_01 = confusion_matrix(y_test, y_pred_ab_01)\n    # show results\n    print(f'''\\nModel a and b results {class_order} class first:\nAccuracy:{accuracy_ab_01 * 100: 0.1f}%\n{matrix_ab_01}''')\n# get result with second class for model b\nclass_in_order_predictor(class_order='second')\n\n# Current chosen class for model b: second\n#\n# Dataframe with prediction of model a:\n# Number of samples: 500\n# Number of features: 20\n# Samples by class:\n# Class=1, Count=238, Percentage=47.6%\n# Class=0, Count=262, Percentage=52.4%\n#\n# Dataframe with prediction of model b:\n# Number of samples: 262\n# Number of features: 20\n# Samples by class:\n# Class=0, Count=225, Percentage=85.9%\n# Class=1, Count=37, Percentage=14.1%\n#\n# Model a and b results second class first:\n# Accuracy: 91.4%\n# [[216  34]\n#  [  9 241]]","metadata":{"execution":{"iopub.status.busy":"2022-06-01T12:22:24.358974Z","iopub.execute_input":"2022-06-01T12:22:24.360487Z","iopub.status.idle":"2022-06-01T12:22:24.497001Z","shell.execute_reply.started":"2022-06-01T12:22:24.360435Z","shell.execute_reply":"2022-06-01T12:22:24.495908Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"Is it possible to achieve an even better result if the models are swapped? Let's find out!","metadata":{}},{"cell_type":"code","source":"print(\"\\nSwap models a and b:\")\nmodel_a = KNeighborsClassifier().fit(X_train, y_train)\nmodel_b = RandomForestClassifier(random_state=1).fit(X_train, y_train)\n# call predictor with independent models for each class\nclass_in_order_predictor(class_order='second')\n\n# Swap models a and b:\n#\n# Current chosen class for model b: second\n#\n# Dataframe with prediction of model a:\n# Number of samples: 500\n# Number of features: 20\n# Samples by class:\n# Class=0, Count=244, Percentage=48.8%\n# Class=1, Count=256, Percentage=51.2%\n#\n# Dataframe with prediction of model b:\n# Number of samples: 244\n# Number of features: 20\n# Samples by class:\n# Class=1, Count=19, Percentage=7.8%\n# Class=0, Count=225, Percentage=92.2%\n#\n# Model a and b results second class first:\n# Accuracy: 91.4%\n# [[216  34]\n#  [  9 241]]","metadata":{"execution":{"iopub.status.busy":"2022-06-01T12:22:24.498802Z","iopub.execute_input":"2022-06-01T12:22:24.499533Z","iopub.status.idle":"2022-06-01T12:22:24.967555Z","shell.execute_reply.started":"2022-06-01T12:22:24.499475Z","shell.execute_reply":"2022-06-01T12:22:24.966297Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"And now let's compare the result with the work of Voting ensembles:","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import VotingClassifier\n# call hard voting ensemble\npredictor(VotingClassifier(estimators=[('a', model_a), ('b', model_b)], voting='hard'))\n\n# VotingClassifier results:\n# Accuracy: 90.2%\n# [[241   9]\n#  [ 40 210]]\n# Processed in  0.1990 seconds","metadata":{"execution":{"iopub.status.busy":"2022-06-01T12:22:24.975492Z","iopub.execute_input":"2022-06-01T12:22:24.976938Z","iopub.status.idle":"2022-06-01T12:22:25.398990Z","shell.execute_reply.started":"2022-06-01T12:22:24.976882Z","shell.execute_reply":"2022-06-01T12:22:25.397991Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"A Hard voting showed exactly the result achieved when giving model \"b\" a class 1 job. Soft voting showed the best result among all models and approaches:","metadata":{}},{"cell_type":"code","source":"# call soft voting ensemble\npredictor(VotingClassifier(estimators=[('a', model_a), ('b', model_b)], voting='soft'))\n\n# VotingClassifier results:\n# Accuracy: 91.6%\n# [[230  20]\n#  [ 22 228]]\n# Processed in  0.2079 seconds","metadata":{"execution":{"iopub.status.busy":"2022-06-01T12:22:25.400299Z","iopub.execute_input":"2022-06-01T12:22:25.400922Z","iopub.status.idle":"2022-06-01T12:22:25.699490Z","shell.execute_reply.started":"2022-06-01T12:22:25.400877Z","shell.execute_reply":"2022-06-01T12:22:25.696286Z"},"trusted":true},"execution_count":11,"outputs":[]}]}